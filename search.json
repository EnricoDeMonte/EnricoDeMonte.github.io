[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Enrico De Monte",
    "section": "",
    "text": "I’m researcher at ZEW - Leibniz Centre for European Economic Research. My research interests lie in the fields of industrial organisation and applied econometrics.\nTopics I work on comprise productivity growth, competition, market power, business dynamism, and innovation. Understanding the interrelation between these concepts provides insights to the evolution of an economy in terms of technological change and the process of job creation and destruction, which plays an important role for our standard of living. Contributing to designing efficient policies to support the latter is the ultimate purpose of my research."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bio",
    "section": "",
    "text": "I’m researcher in the field of industrial organisation."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Peer-reviewed publications\nNonparametric instrumental regression with two-way fixed effects, 2023, Journal of Econometric Methods. [Code]\n\n\nMore\n\n\nThis paper presents a novel nonparametric instrumental estimator while controlling for unobserved additive fixed effects. In particular, I consider a model such as\n\\[\nY_{it} = \\varphi(Z_{it}) + \\xi_{i} + \\delta_t + U_{it},\n\\]\nwhere \\(Z_{it}\\) is supposed to be correlated with both the unobserved individual and temporal effects, \\(\\xi_i\\) and \\(\\delta_t\\), and with the the error term \\(U_{it}\\). Such settings are typical when estimating market equilibrium models as, for instance, product demand or supply, where \\(Y_{it}\\) and \\(Z_{it}\\) might represent output prices and quantity and \\(\\xi_i\\) and \\(\\delta_t\\) unit and time specific shocks. To consistently estimate the nonparametric conditional mean function \\(\\varphi\\) one has to control for the simultaneity bias, occurring when prices and output quantity are jointly determined, as well as for the unobserved fixed effects. For that purpose, I combine the Landweber-Fridman regularization for the IV-part with a locally-weighted nonparametric fixed effects estimator. A Monte Carlo simulation reveals good finite sample behavior of the novel estimator and confidence intervals are provided by the application of the wild residual block bootstrap.\n\n\n\n\nThe figure shows the regularized solution path of the nonparametric instrumental estimator while controling for two-way fixed effects. The estimator is applied on simulated data where the conditional mean function \\(\\varphi\\) is consistently estimated after 44 iterations, depicted by the green line.\n\n\n\n\n\n\nProductivity dynamics and exports in the French forest product industry. 2022, Journal of Forest Economics, 37(1), 1-71. [Working paper version]\n\n\nMore\n\n\nThis paper investigates aggregate productivity dynamics of the French forest product industry based on firm-level data from 1994 to 2016. The main objectives of the paper are to investigate aggregate productivity growth in the industry, while taking market entry and exit into account. Further, aggregate productivity growth is investigated with respect to firms’ export status and with respect to their domestic and export economic activity. Decomposing the productivity growth into the contribution of incumbent, entering, and exiting firms, the results show a considerable slowdown during the economic crisis from 2007 on, which is mainly induced by decreasing productivity improvements and inefficient resource allocation among incumbent firms. Moreover, the study shows that exporters contribute more to aggregate productivity growth than non-exporters. However, investigating the contribution of firms’ domestic and export economic activities on aggregate productivity growth, I find that the aggregate productivity growth is mainly related to firms’ domestic economic activity.\n\n\n\n\n\n\nThe figure shows the empirical cumulative distribution function (ECDF) of exporting and non-exporting firms’ productivity. For all periods exporters of the French forest product industry reveal higher productivity compared to non-exporting firms.\n\n\n\n\n\n\nWorking papers\nProductivity, markups, and reallocation: Evidence from French manufacturing firms from 1994 to 2016 (R&R to Journal of Productivity Analysis)\n\n\nMore\n\n\nThis paper investigates the evolution of aggregate productivity and markups among French manufacturing firms between 1994 and 2016, by focusing on the role of reallocation with respect to both aggregate measures. Firm-level productivity and markups are estimated based on a gross output translog production function using popular estimation methods.\n\n\n\n\n\n\nAs the figures show, I find an aggregate productivity growth of about 34% over the whole period while aggregate markups are found to remain relatively stable (see the solid lines in both figures). As a key finding the study further shows that over time reallocation of sales shares affects differently aggregate productivity and markups (long-dashed line): Before 2000 both aggregate productivity and markups are importantly driven by reallocation effects; Post-2000, instead, the contribution of reallocation to aggregate productivity becomes negligible, inducing a slowdown in aggregate productivity growth, while I measure persistent reallocation of sales shares from lower to higher markup firms. Policy relevant implications of these dynamics are discussed.\n\n\n\n\nCournot equilibrium and welfare with heterogenous firms (with B. Koebel)\n\n\nMore\n\nSoon to be updated.\n\n\nWork in progress\nBusiness dynamism and high-growth firms in Germany (with S. Gottschalk, J. Miranda, and S. Murmann)\nIs age golden? Growth, innovation, and survival of start-ups by seniorpreneuers (with S. Gottschalk H. Hottenrott, S. Murmann, E. Rodepeter)\n\n\n\nPolicy advice and technical reports\nEx-ante analysis of the German start-up subsidy Programm “INVEST” (with various ZEW researchers and Technopolis Deutschland GmbH, on behalf of the German Federal Ministry of Economic Affairs and Climate Action, 2022)\nThe effect of the German minimum wage on conditions of competition (with A. Kann, M. Lubczyk and S. Murmann, on behalf of the German minimum wage commission, 2022)\n\nMedia coverage: Tagesschau, Süddeutsche Zeitung\n\nIndustry and productivity dynamics in Germany (with J. Bersch, N. Hahn, and G. Licht, ZEW research project for the Bertelsmann Stiftung, 2021)\n\nMedia coverage: Frankfurter Allgemeine Zeitung\n\n\n\nBook chapters\nDynamisierung der Wirtschaft: Eine Herausforderung für die Industriepolitik, Deutsch-Französisches Institut (dfi), Frankreich Jahrbuch 2022 - Politik der Zeitenwende? Europa im Umbruch, Frankreich Jahrbuch, Bd. 36 , Nomos Verlagsgesellschaft 69-88 (with G. Licht)"
  },
  {
    "objectID": "research.html#peer-reviewed-publications",
    "href": "research.html#peer-reviewed-publications",
    "title": "Research",
    "section": "Peer reviewed publications",
    "text": "Peer reviewed publications\nProductivity dynamics and exports in the French forest product industry. 2022, Journal of Forest Economics, 37(1), 1-71.\nNonparametric instrumental regression with two-way fixed effects, 2023, Journal of Econometric Methods."
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "Working papers",
    "text": "Working papers\nProductivity, markups, and reallocation: Evidence from French manufacturing firms from 1994 to 2016} (R&R to Journal of Productivity Analysis)\nCournot equilibrium and welfare with heterogenous firms (with B. Koebel)\nBusiness dynamism and high-growth firms in Germany (with S. Gottschalk and J. Miranda)\nIs age golden? Growth, innovation, and survival of start-ups by seniorpreneuers} (with S. Gottschalk, H. Hottenrott, S. Murmann, E. Rodepeter)"
  },
  {
    "objectID": "research.html#technical-reports-and-policy-advice",
    "href": "research.html#technical-reports-and-policy-advice",
    "title": "Research",
    "section": "Technical reports and policy advice",
    "text": "Technical reports and policy advice\nIndustry and productivity dynamics in Germany}, (with J. Bersch, N. Hahn, and G. Licht, ZEW research project for the Bertelsmann Stiftung, 2021)\nEx-ante analysis of the German start-up subsidy Programm “INVEST”, (with various ZEW researchers and Technopolis Deutschland GmbH, on behalf of the German Federal Ministry of Economic Affairs and Climate Action, 2022)\nThe effect of the German minimum wage on conditions of competition, (with A. Kann, M. Lubczyk, and S. Murmann, on behalf of the German minimum wage commission, 2022)"
  },
  {
    "objectID": "research.html#book-chapeters",
    "href": "research.html#book-chapeters",
    "title": "Research",
    "section": "Book chapeters",
    "text": "Book chapeters\nDynamisierung der Wirtschaft: Eine Herausforderung für die Industriepolitik Deutsch-Französisches Institut (dfi) Frankreich Jahrbuch 2022 - Politik der Zeitenwende? Europa im Umbruch, Frankreich Jahrbuch, Bd. 36 , Nomos Verlagsgesellschaft 69-88 (with G. Licht)"
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Projects",
    "section": "",
    "text": "Funding and researchers\n\n\nThis project will be realized as a cooperation between IWH - Halle Institute for Economic Research and ZEW - Leibniz Centre for European Economic Research.\nThe project is funded by the Leibniz Association, by obtaining the Leibniz Collaborative Excellence fund as part of the Leibniz Competition, which comprises for both institutes 1 Mio. Euro over three years (2024-2026).\nProject lead/speakers\n\nEnrico De Monte\nJavier Miranda\n\nPrincipal investigators\n\nProf. Dr. Ufuk Akcigit, University of Chicago,\nDr. Andre Diegmann, IWH,\nProf. Dr. Karin Hoisl, University of Mannheim,\nProf. Dr. Hanna Hottenrott, ZEW and Technical University Munich\nProf. Dr. Javier Miranda, IWH and Friedrich-Schiller University Jena\nDr. Enrico De Monte, ZEW\nProf. Dr. Merih Sevilir, IWH and ESMT-Berlin\n\n\n\n\n\nShort summary\n\n\nTransformative innovation is fundamentally disruptive leading to reallocation, productivity growth, and growing standards of living. Entrepreneurial firms are believed to be at the heart of this process by introducing new ideas, products, and services that displace those offered by less innovative firms. This creative-destruction process underlies business dynamism in modern market economies. However, entrepreneurship and business dynamism are in decline in the US and Europe with potentially broad implications for innovation, productivity growth, and well-being. The arising key question is what can be done to achieve sustainable competitiveness. We propose a program to study the conditions, determinants, and implications of innovative high-growth entrepreneurship in Germany. We bring together a leading group of national and international experts in a partnership between IWH and ZEW and develop a rich new data infrastructure to study high-growth entrepreneurship.\nThe work program consists of three work packages (WP). The first one develops the microdata infrastructure necessary to study entrepreneurship and business dynamism in Germany. In particular, we bring together detailed information of firms, establishments, founders, workers, and inventions. The second WP aims to shed light on high-growth firm activity and its relation to innovation. We pay special attention to: i) who creates high-growth firms looking at the role of entrepreneurial traits; and ii) who works at high growth firms looking at the role of workforce skill and shortages. The third WP investigates the role of the competitive environment and the role of acquisitions. The output of this project will be threefold: First, new insights with high relevance for policymakers in Germany and beyond. Second, a new and lasting data infrastructure that will allow high-quality research and policy advice. Third, this project will strengthen the relations and collaboration between the Leibniz institutes IWH and ZEW.\n\n\n\n\n\n\nSeptember, 22nd 2023\nWinning the Leibniz Competition Fund for the 3-year research project “High Growth Entrepreneurship, Innovation, and the Transformation of the Economy.\n\n\nMay, 30th 2023\nSubmission of research proposal to the Leibniz Association for participating in the Leibniz Competition."
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Download here my CV."
  },
  {
    "objectID": "research.html#work-in-progress",
    "href": "research.html#work-in-progress",
    "title": "Research",
    "section": "Work in progress",
    "text": "Work in progress\nBusiness dynamism and high-growth firms in Germany (with S. Gottschalk and J. Miranda)\nIs age golden? Growth, innovation, and survival of start-ups by seniorpreneuers} (with S. Gottschalk H. Hottenrott, S. Murmann, E. Rodepeter)"
  },
  {
    "objectID": "project.html#hallo-1",
    "href": "project.html#hallo-1",
    "title": "Projects",
    "section": "Hallo",
    "text": "Hallo\n\nkjkjkj"
  },
  {
    "objectID": "project.html#general-information",
    "href": "project.html#general-information",
    "title": "Projects",
    "section": "",
    "text": "Funding and researchers\n\n\nThis project will be realized as a cooperation between IWH - Halle Institute for Economic Research and ZEW - Leibniz Centre for European Economic Research.\nThe project is funded by the Leibniz Association, by obtaining the Leibniz Collaborative Excellence fund as part of the Leibniz Competition, which comprises for both institutes 1 Mio. Euro over three years (2024-2026).\nProject lead/speakers\n\nEnrico De Monte\nJavier Miranda\n\nPrincipal investigators\n\nProf. Dr. Ufuk Akcigit, University of Chicago,\nDr. Andre Diegmann, IWH,\nProf. Dr. Karin Hoisl, University of Mannheim,\nProf. Dr. Hanna Hottenrott, ZEW and Technical University Munich\nProf. Dr. Javier Miranda, IWH and Friedrich-Schiller University Jena\nDr. Enrico De Monte, ZEW\nProf. Dr. Merih Sevilir, IWH and ESMT-Berlin\n\n\n\n\n\nShort summary\n\n\nTransformative innovation is fundamentally disruptive leading to reallocation, productivity growth, and growing standards of living. Entrepreneurial firms are believed to be at the heart of this process by introducing new ideas, products, and services that displace those offered by less innovative firms. This creative-destruction process underlies business dynamism in modern market economies. However, entrepreneurship and business dynamism are in decline in the US and Europe with potentially broad implications for innovation, productivity growth, and well-being. The arising key question is what can be done to achieve sustainable competitiveness. We propose a program to study the conditions, determinants, and implications of innovative high-growth entrepreneurship in Germany. We bring together a leading group of national and international experts in a partnership between IWH and ZEW and develop a rich new data infrastructure to study high-growth entrepreneurship.\nThe work program consists of three work packages (WP). The first one develops the microdata infrastructure necessary to study entrepreneurship and business dynamism in Germany. In particular, we bring together detailed information of firms, establishments, founders, workers, and inventions. The second WP aims to shed light on high-growth firm activity and its relation to innovation. We pay special attention to: i) who creates high-growth firms looking at the role of entrepreneurial traits; and ii) who works at high growth firms looking at the role of workforce skill and shortages. The third WP investigates the role of the competitive environment and the role of acquisitions. The output of this project will be threefold: First, new insights with high relevance for policymakers in Germany and beyond. Second, a new and lasting data infrastructure that will allow high-quality research and policy advice. Third, this project will strengthen the relations and collaboration between the Leibniz institutes IWH and ZEW."
  },
  {
    "objectID": "project.html#news-blog",
    "href": "project.html#news-blog",
    "title": "Projects",
    "section": "",
    "text": "September, 22nd 2023\nWinning the Leibniz Competition Fund for the 3-year research project “High Growth Entrepreneurship, Innovation, and the Transformation of the Economy.\n\n\nMay, 30th 2023\nSubmission of research proposal to the Leibniz Association for participating in the Leibniz Competition."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "E-Mail and phone:\n\nYou can reach me at enrico.demonte [at] zew.de or on the phone +49 621 1235-284.\n\nOffice:\n\nZEW - Leibniz Centre for European Economic Research\nL7,1\nDE-68161 Mannheim\nGermany"
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "Code and Packages",
    "section": "",
    "text": "Nonparametric instrumental regression with two-way fixed effects, 2023, Journal of Econometric Methods.\n\n\nDownload the file npivfe containing an R file with all functions needed to apply the approach described in the paper as well as a readme file providing a description of the functions.\n\n\n\nBelow we generate data of the two-way fixed effects panel model, given by \\[ Y_{it} = \\varphi(X_{it}) + \\xi_i + \\delta_t + U_{it},\\] where \\(X_{it}\\) denotes the endogenous dependent variable that is correlated with both the unobserved individual and temporal fixed effects, \\(\\xi_i\\) and \\(\\delta_t\\), and with the error term \\(U_{it}\\). Further, we generate a valid instrument \\(Z_{it}\\), satisfying \\[\nE(U_{it} | Z_{it}) = 0 \\hspace{4mm} \\text{and} \\hspace{4mm} cor(X_{it},Z_{it}) \\neq 0.  \n\\] Note that to stay consistent with the variable convention used in the code, the notation used here changes slightly w.r.t. the one presented in paper. More precisely, while in the paper \\(Z_{it}\\) and \\(W_{it}\\) denote the explanatory and the instrumental variable, in the code these variables refer to x and z, respectively.\n\n\n\n\nlibrary(knitr)\nlibrary(latex2exp)\n\nset.seed(49)\n\n# Generate panel structure \nN = 100  \nT = 20   \nNT = N*T\niota_T = rep(1, T)\niota_N = rep(1, N)\ntrend_T = c(1:T)\nind_N = c(1:N)\ncol_T = iota_N %x% trend_T \ncol_N = ind_N %x% iota_T \n\n# Parts of the DGP in Racine (2019, p. 288) \n# Correlation parameters \nrho.xz = 0.2  \nrho.ux = 0.8\nsigma.u = 0.05\n\n# DGP of the quadratic model \ndgp = function(x){x^2}\n\n# Generate endogeneity in x \nv1 = rnorm(NT, 0, 0.8)\nv2 = rnorm(NT, 0, 0.15)\neps = rnorm(NT, 0, sigma.u)\nu = rho.ux*v1 + eps \n\n# Individual fixed effects\nxi_i = runif(N, 0, 1.5)\nxi_i = as.matrix( xi_i %x% iota_T )\n\n# Time fixed effects \ndelta_t = runif(T, 0, 1.7)\ndelta_t = as.matrix( rep( delta_t, N ) )\n\n# Explanatory variable needs to be correlated with\n# the error term as well as with the\n# individual and temporal effects\n\n# Generate \"auxiliary\" explanatory variable\nx = matrix( NT, nrow = NT, ncol = 1 )\n\n# Correlation with individual and temporal effects\nfor(i in 1:NT){\n  x[i,1] = rnorm( 1, mean = (xi_i[i] + delta_t[i]), sd = 1 ) \n}\n\n# Auxiliary variable used to introduce correlation \n# between the instrument and the explanatory variable\nz = rho.xz*x + v2\n\n# Generate explanatory variable also correlated\n# with error term (i.e. E(u*x) != 0)\nx = x + v1\n\n# Dependent variable \ny = as.matrix( dgp(x=x) ) + xi_i + delta_t + u \n\n# Plot the data points along with the true DGP\nplot( x, y,\n      pch = 21, \n      col = \"grey\", \n      ylab = \"Y\", \n      xlab = \"X\" ,\n      main = \"\")\ncurve( dgp(x),\n       col=\"red\", \n       add=TRUE, \n       lty = 2, \n       lwd = 2)\nlegend( -2, 40, \n        legend = c(\"DGP\", \"Observations\"), \n        col = c(\"red\", \"grey\"),\n        lty = c(2, 1),\n        lwd = c(2, 2),\n        cex = 0.8)\n\n\n\n# Covariance matrix \ncovmat = round( cov( cbind( x, z, xi_i, delta_t, u ) ), 2)\ncolnames( covmat ) &lt;- rownames( covmat ) &lt;- c( \"$x_{it}$\", \"$z_{it}$\", \"$\\\\xi_i$\", \"$\\\\delta_t$\", \"$u_{it}$\" )\ncovmat[ lower.tri( covmat, diag = TRUE ) ] =  \"\"\nkable( covmat, \n       caption = \"Covariance matrix of observables and unobservables\" )\n\n\nCovariance matrix of observables and unobservables\n\n\n\n\\(x_{it}\\)\n\\(z_{it}\\)\n\\(\\xi_i\\)\n\\(\\delta_t\\)\n\\(u_{it}\\)\n\n\n\n\n\\(x_{it}\\)\n\n0.27\n0.19\n0.25\n0.52\n\n\n\\(z_{it}\\)\n\n\n0.04\n0.05\n0\n\n\n\\(\\xi_i\\)\n\n\n\n0\n-0.01\n\n\n\\(\\delta_t\\)\n\n\n\n\n0.01\n\n\n\\(u_{it}\\)\n\n\n\n\n\n\n\n\n\n\nTo further investigate the data you might assemble the observable variables with the individual and time identifiers:\n\nData = data.frame( cbind( col_N, col_T, y, x, z ) )\ncolnames( Data ) &lt;- c( \"col_N\", \"col_T\", \"y\", \"x\", \"z\" )\n\n\n\n\nLoad the functions needed for the nonparametric instrumental estimator with two-way fixed effects. (Download above the file npivfe.R containing the functions.)\n\nsource(\".../npivfe.R\")\n\nUse the function npregivfe provided in the package to estimate the conditional mean function \\(\\varphi\\).\nNote that the estimation routine is very time intensive as it involves several numerical optimizations to obtain optimal bandwidths as well as several iterations of the Landweber-Fridman regularization procedure. However, I’m sure that the way I coded the estimator can be improved significantly to reduce the run time.\n\nres.npivfe = npregivfe( y = y,\n                        x = x,\n                        z = z, \n                        c = 0.5,  \n                        tol = 1,   \n                        N = N,\n                        T = T, \n                        max.iter = 100,\n                        bw_method = \"optimal\",\n                        effects = \"indiv-time\",\n                        type = \"ll\" )\n\nInvestigate the regularized solution path:\n\nlibrary(latex2exp)\nlibrary(colorRamps)\n# (NT x K) matrix, with K the total number of iterations\n# within the Landweber-Fridman estimation procedure. \nphi_k_mat = res.npivfe[[1]]\n\n# Total number of iterations\nk_bar = ncol( phi_k_mat ) \n\n# Final point-wise estimates of the function of interest\nfit.npivfe = phi_k_mat[, k_bar]  \n\n# Plot the initial guess phi_0\nplot( x[order(x)], phi_k_mat[, 1][order(x)],\n      col = blue2green(k_bar)[1],\n      type = \"l\",\n      lwd = 2,\n      ylim = range(y),\n      ylab = \"Y\", xlab = \"X\", main = \"\" ) \n\n# Plot all successive estimates phi_k, k=1,...,k_bar\nfor(k in 1:k_bar){\n  lines( x[order(x)], phi_k_mat[, k][order(x)], \n         type = \"l\",\n         lwd = 2, \n         col =  blue2green(k_bar)[k] )\n}\n# Add the true curve  \ncurve( dgp, \n       min(x), max(x),\n       add = TRUE, \n       col = \"red\", \n       lwd = 2, lty = 2)\nlegend( -2, 40, \n        legend = c(\"DGP\", TeX( '$\\\\hat{\\\\varphi}(x)_{0}$'), \n                  TeX('$\\\\hat{\\\\varphi}(x)_{\\\\bar{k}}$')), \n        col = c(\"red\",blue2green(k_bar)[1], blue2green(k_bar)[k_bar]),\n        lty = c(2, 1, 1),\n        lwd = c(2, 1, 1),\n        cex = 0.65)\n\n\nInvestigate the evolution of the stopping criterion. Note that the number of iterations of the Landweber-Fridman procedure not only vary when changing the constant c but also for some numerical reasons (bandwidths selection). Even though I here also specified c = 0.5, the number of iterations is slightly higher, given by 48 (44 in the paper).\n\n# Store the values of the stopping criterion\nval_crit_k = res.npivfe[[2]]\n\n# Plot the stopping criterion \nplot( 1:k_bar,\n      val_crit_k, type = \"l\", \n      col = \"black\",\n      lwd = 2, \n      main = \"\", \n      xlab = \"k-iterations\",\n      ylab= TeX('$Criterion_k $'))\npoints( k_bar, val_crit_k[k_bar] )\n\n\n\n\n\n1. Local-linear kernel regression (LL)\nUse the functions npregbw and npreg from the np package to estimate the model by a nonparametric local-linear kernel regression.\n\nlibrary(np)\n# Compute optimal bandwidth \nfit.np_bw = npregbw( xdat = as.vector(x), ydat = as.vector(y), \n                     ckertype=\"gaussian\", \n                     regtype = \"ll\", \n                     bwmethod = \"cv.ls\")\n# Obtain fitted values \nfit.np = fitted( npreg( fit.np_bw ) )\n\n2. Nonparametric locally weighted fixed effects estimator (LMU)\nFirst, use the function LMU.CVCMB from the provided package to obtain optimal bandwidths, using leave-one-out conditional mean based cross-validation (CVCMB). Second, use the function LMU.estimator (Lee et al. 2019) with the previously obtained bandwidths to estimate the function of interest.\n\n# Compute optimal bandwidth\nh_LMU = optim( par = sqrt( var(x) )*NT^(-1/7),\n               LMU.CVCMB, \n               method = \"Nelder-Mead\",\n               control = list(reltol=0.001), \n               x = x,\n               y = y,\n               N = N,\n               T = T,\n               effects = \"indiv-time\",\n               type = \"ll\",\n               jump = 1, \n               c1 = 1/T^2, c2 = 1/((N*T)^2) )$par\n\n# Obtain fitted values\nfit.LMU = LMU.estimator( h = h_LMU,\n                         x = x,\n                         y = y,\n                         N = N,\n                         T = T,\n                         effects = \"indiv-time\",\n                         type = \"ll\",\n                         c1 = c1, c2 = c2)[ , \"mhat\"]\n\n3. Nonparametric IV using the Landweber-Fridman regularization (L-F)\nUse the function npregiv2 from the provided package to estimate the model by nonparametric IV regression (without fixed effects) applying the Landweber-Fridman (L-F) regularization. Note that the function relies on the np package (as it calls the functions npreg and npregbw) and produces very similar results compared to the np::npregiv function (also see Florens et al. 2018).\n\n# Nonparametric IV (without fixed effects)\nres.npivf2 = npregivf2( y = y,\n                        z = x, \n                        w = z, \n                        c = 0.5,\n                        tol=1,\n                        max.iter = 100,\n                        bw_method = \"optimal\" )\n\n# Obtain final estimates by selecting the last column (last iteration)\nfit.npivf2 = res.npivf2[[1]][, ncol( res.npivf2[[1]] ) ]\n\nPlot the results to compare the different estimators.\n\nplot( x, y, \n      pch = 21, \n      col = \"grey\", \n      ylab = \"Y\", \n      xlab = \"X\" ,\n      main = \"\")\nlines( x[order(x)], fit.np[order(x)],\n       col= \"orange\",\n       type = \"l\",\n       lty =1,\n       lwd = 2)\nlines( x[order(x)], fit.npiv2[order(x)],\n       col= \"black\",\n       type = \"l\",\n       lty = 1,\n       lwd = 2)\nlines( x[order(x)], fit.LMU[order(x)],\n       col= \"darkblue\",\n       type = \"l\",\n       lty = 1,\n       lwd = 2)\nlines( x[order(x)], fit.npivfe[order(x)],\n       type = \"l\",\n       col = blue2green(k_bar)[k_bar], \n       lty =1, \n       lwd = 2)\ncurve( dgp(x),\n       col=\"red\", \n       add=TRUE, \n       lty = 2, \n       lwd = 2)\nlegend( -1,40, \n        legend = c(\"DGP\", \"local-linear\", \"L-F\", \"LMU\", \"L-F/LMU\"), \n        col = c(\"red\", \"orange\", \"black\" , \"darkblue\", blue2green(k_bar)[k_bar]),\n        lty = c(2,1,1,1,1), \n        lwd = c(2,2,2,2,2), \n        cex = 0.65 )\n\n\n\n\n\nAs proposed in the paper, bootstrapped confidence intervals can be obtained by the application of the wild residual block bootstrap following Malikov et al. (2020) and Azomahou et al. (2006). See Appendix B in the paper for more details. The chunk code provided below shows how the estimates were obtained. Note again that the estimation routine is very time intensive. Hence, parallelizing and/or using more computers (as I did) would be useful speed up the routine.\n\n# Use bandwidths from initial estimation of the conditional mean. \nbws = res.npivfe[[3]]\n\n# Compute residuals u_hat \nv_hat = y - fit.npivfe \n\n# Center residuals u_hat\nv_hat_c = as.numeric(scale(v_hat, center = TRUE))\n\n# Create empty lists to store bootstrap estimates\nphi_hat_boot_list &lt;- list()\n\n# Bootstrap iterations \nB = 400\nfor(b in 1:B){\n  \n  # Generate bootstrap weights \n  # each individual i keeps its weight for all t\n  b_i = sample( x = c( (1+sqrt(5))/2, (1-sqrt(5))/2 ), \n                size = N, \n                prob = c( ( sqrt( 5 ) - 1 ) / ( 2 * sqrt( 5 ) ), ( sqrt( 5 )+1 ) / ( 2*sqrt( 5 ) ) ),\n                replace = TRUE )\n  \n  # Generate new disturbances\n  v_hat_b = (b_i %x% iota_T) * v_hat_c\n  \n  # Generate new outcome variable\n  y_b = phi_hat + v_hat_b\n  \n  # Re-estimate phi_hat using y_b \n  res.npivfe_b = npregivfe( y = y_b, \n                            x = x, \n                            z = z, \n                            N = N, \n                            T = T,\n                            bw_method = \"plug_in\", \n                            effects = \"indiv-time\", \n                            type = \"ll\", \n                            c = 0.5,\n                            tol = 1, \n                            bws = bws,\n                            max.iter = 100)\n  \n  phi_hat_b = res.npivfe_b[[1]][, ncol( res.npivfe_b[[1]] ) ]\n  \n  phi_hat_boot_list[[b]] &lt;- phi_hat_b\n}\n\n# Bind the results in a (NT x B) matrix.  \nphi_boot = do.call( cbind, phi_hat_boot_list )\n\n# Compute the point-wise 95% confidence intervals \nphi_025 = apply( phi_boot, 1, quantile, probs=0.025 )\nphi_975 = apply( phi_boot, 1, quantile, probs=0.975 )\n\n# Plot the estimates along with the confidence intervals \nplot( x[order(x)], fit.npivfe[order(x)],\n      ylim = c(0, 40),\n      type = \"l\",\n      lty = 1,\n      lwd = 2,\n      ylab = \"Y\",\n      xlab = \"X\",\n      main = \"\")\nlines( x[order(x)], phi_025[order(x)],\n       lty = 2, \n       col = \"black\")\nlines( x[order(x)], phi_975[order(x)],\n       lty = 2,\n       col = \"black\")\nlegend( -1, 35, \n        legend = c(TeX( '$\\\\hat{\\\\varphi}(x)_{\\\\bar{k}}$'), \"95 % CI\"), \n        col = c(\"black\", \"black\"),\n        lty = c(1, 2, 2), lwd = c(2, 1, 1),\n        cex = 0.8 )\n\n\n\n\n\nLastly, as shown in the paper, a Monte Carlo simulation demonstrates the finite sample properties of the proposed estimator in comparison with other nonparametric estimators, such as those already presented above (i.e. the LL, LMU, and the L-F estimator). The code below shows how the results were obtained. Running the code, however, is very time intensive, which unfortunately renders the reproduction of the results very costly.\nFollowing Lee et al. (2020), the Monte Carlo simulation is performed for four different data sets with \\(N\\in \\{50,100\\}\\) and \\(T\\in\\{10,20\\}\\). Each of the estimator is applied on each data set 400 times. To reduce run time, this is done by fixing bandwidths to the ones obtained from the first iteration. Subsequently, the Root Mean Squared Error (RMSE) and the Integrated Mean Absolute Error (IMAE) are computed.\nThe function dgpivfe provided in the package allows to generate the above described data by specifying the length of the individual and time dimension, N and T, yielding a data.table object that contains the observables y, x, and z.\n\n# Vectors defining the length of the data sets\nno_indiv = c(50, 50, 100, 100)\nno_time =  c(10, 20, 10, 20)\n\n# Generate matrices to store the values of the RMSE and the IMAE\nSim_res_RMSE_1 = Sim_res_RMSE_2 = Sim_res_RMSE_3 = Sim_res_RMSE_4 = matrix( 0, nrow = rep, ncol = 4 )\ncolnames(Sim_res_RMSE_1) = colnames(Sim_res_RMSE_2) = colnames(Sim_res_RMSE_3) = colnames(Sim_res_RMSE_4) &lt;- c(\"LF/LMU\", \"LF\", \"LMU\", \"LL\")\n\nSim_res_IMAE_1 = Sim_res_IMAE_2 = Sim_res_IMAE_3 = Sim_res_IMAE_4 = matrix( 0, nrow = rep, ncol = 4 )\ncolnames(Sim_res_IMAE_1) = colnames(Sim_res_IMAE_2) = colnames(Sim_res_IMAE_3) = colnames(Sim_res_IMAE_4) &lt;- c(\"LF/LMU\", \"LF\", \"LMU\", \"LL\")\n\ndgp = function(x){ x^2 }\n\n# Monte Carlo repetitions\nrep = 400\n\nfor(d in 1:4){\n  for(i in 1:rep){ \n  # Specify length of the data\n  N = no_indiv[d]; T = no_time[d] ; NT = N*T\n  # Draw the data \n  Data = dgpivfe(N=N, T=T)\n  \n  # 1) The LF/LMU estimator (IV with two-way fixed effects)\n  if(i == 1){ # Bandwidths are fixed to those obtained of the first repetition \n    res.npivfe = npregivfe( y = Data$y,\n                            x = Data$x, \n                            z = Data$z,\n                            N = N,\n                            T = T,\n                            c = 0.8, \n                            tol = 1,\n                            max.iter = 100,\n                            bw_method = \"optimal\" )\n    bws_LF_LMU = phi_hat_LF_LMU[[3]]\n    fit.npivfe =res.npivfe[[1]][, ncol( res.npivfe[[1]] ) ]\n  }else{\n    res.npivfe = res.npivfe( y = Data$y,\n                             x = Data$x,\n                             z = Data$z,\n                             N = N, \n                             T = T,\n                             c = 0.8,\n                             tol=1,\n                             max.iter=100,\n                             bw_method = \"plug_in\",\n                             bws = bws_LF_LMU )\n  }\n  # 2) The LF estimator (nonparametric IV only)\n  if(i == 1){\n    res.npiv2 = npregiv2( y = Data$y,\n                          z = Data$x, \n                          w = Data$z, \n                          c = 0.8,\n                          tol=1,\n                          max.iter = 100,\n                          bw_method = \"optimal\" )\n    bws_LF = res.npiv2[[3]]\n    fit.npivf2 = res.npivf2[[1]][, ncol( res.npivf2[[1]] ) ]\n  }else{\n    res.npiv2 = npivf2( y = Data$y,\n                        z = Data$x,\n                        w = Data$z,\n                        c = 0.8, \n                        tol=1,\n                        max.iter = 100,\n                        bw_method = \"plug_in\",\n                        bws = bws_LF  )\n    fit.npivf2 = res.npivf2[[1]][, ncol(res.npivf2[[1]])]\n    }\n\n  # The LUM estimator (nonparametric two-way fixed effects) \n  if(i==1){\n  bw_LMU = optimize( f = Lee.CVCMB,\n                     interval = c(0,5*sd(Data$x)),\n                     lower = 0,\n                     upper = 5*sd(Data$x),\n                     tol = 0.001,\n                     x = Data$x,\n                     y = Data$y,\n                     N = N,\n                     T = T,\n                     effects = \"indiv-time\",\n                     type = \"ll\",\n                     jump = 1,\n                     c1 = 1/T^2, c2 = 1/((N*T)^2))$minimum\n  \n   fit.LMU = LMU.estimator( h = bw_LMU,\n                            x = Data$x,\n                            y = Data$y,\n                            N = N,\n                            T = T,\n                            effects = \"indiv-time\",\n                            type = \"ll\",\n                            c1 = 1/T^2, c2 = 1/((N*T)^2))[,\"mhat\"]\n  }else{\n   fit.LMU = LMU.estimator( h = bw_LMU,\n                            x = Data$x, \n                            y = Data$y,\n                            N = N,\n                            T = T,\n                            effects = \"indiv-time\",\n                            type = \"ll\",\n                            c1 = 1/T^2, c2 = 1/((N*T)^2))[,\"mhat\"]\n  }\n\n  # The LL estimator (local-linear kernel regression) \n  if(i==1){\n    bw_ll = npregbw( Data$y~Data$x,\n                     ckertype=\"gaussian\" ,\n                     regtype = \"ll\",\n                     bwmethod = \"cv.ls\")$bw \n    fit.LL = fitted( npreg( bws = bw_ll,\n                            tydat = Data$y, \n                            txdat = Data$x ) ) \n  }else{\n    fit.LL = fitted( npreg( bws = bw_ll,  \n                            tydat = Data$y,\n                            txdat = Data$x,\n                            ckertype=\"gaussian\"))\n  }\n  if(d == 1){ \n    # RMSE \n    Sim_res_RMSE_1[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_1[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_1[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_1[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_1[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_1[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_1[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_1[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n  if(d==2){\n    # RMSE \n    Sim_res_RMSE_2[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_2[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_2[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_2[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_2[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_2[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_2[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_2[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n  if(d==3){\n    # RMSE \n    Sim_res_RMSE_3[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_3[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_3[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_3[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_3[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_3[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_3[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_3[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n  if(d==4){\n    # RMSE \n    Sim_res_RMSE_4[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_4[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_4[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_4[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_4[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_4[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_4[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_4[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n\n }\n}\n\n# Output table of the MC simulation\ntab.MC = cbind(no_indiv, no_time, rbind( round( colMeans( Sim_res_RMSE_1 ), 3 ), \n                                         round( colMeans( Sim_res_RMSE_2 ), 3 ),\n                                         round( colMeans( Sim_res_RMSE_3 ), 3 ), \n                                         round( colMeans( Sim_res_RMSE_4 ), 3 ) ),\n                                  rbind( round( colMeans( Sim_res_IMAE_1 ), 3 ), \n                                         round( colMeans( Sim_res_IMAE_2 ), 3 ),\n                                         round( colMeans( Sim_res_IMAE_3 ), 3 ), \n                                         round( colMeans( Sim_res_IMAE_4 ), 3 ) ) )\n\ncolnames(tab.MC)[1:2] &lt;- c(\"N\", \"T\")\n\nThe table tab.MC provides the results of the Monte Carlo simulation as shown below, where the columns 3 to 6 and 7 to 10 refer to the RMSE and the IMAE, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(N\\)\n\\(T\\)\nL-F/LMU\nL-F\nLMU\nLL\nL-F/LMU\nL-F\nLMU\nLL\n\n\n\n\n50\n10\n1.041\n1.664\n1.501\n1.793\n0.350\n1.606\n0.781\n1.675\n\n\n50\n20\n0.309\n1.660\n0.894\n1.797\n0.123\n1.604\n0.760\n1.678\n\n\n100\n10\n0.822\n1.670\n1.258\n1.778\n0.250\n1.609\n0.865\n1.660\n\n\n100\n20\n0.524\n1.663\n0.915\n1.786\n0.116\n1.608\n0.717\n1.668\n\n\n\n\n\n\n\nAzomahou, T., Laisney, F., & Van, P. N. (2006). Economic development and CO2 emissions: A nonparametric panel approach. Journal of Public Economics, 90(6-7), 1347-1363.\nFlorens, J. P., Racine, J. S., & Centorrino, S. (2018). Nonparametric instrumental variable derivative estimation. Journal of Nonparametric Statistics, 30(2), 368-391.\nLee, Y., Mukherjee, D., & Ullah, A. (2019). Nonparametric estimation of the marginal effect in fixed-effect panel data models. Journal of Multivariate Analysis, 171, 53-67.\nMalikov, E., Zhao, S., & Kumbhakar, S. C. (2020). Estimation of firm‐level productivity in the presence of exports: Evidence from China’s manufacturing. Journal of Applied Econometrics, 35(4), 457-480.\nRacine, J. S. (2019). An introduction to the advanced theory of nonparametric econometrics: A replicable approach using R. Cambridge University Press."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "test",
    "section": "",
    "text": "This extension allows you to use [academicons](https://jpswalsh.github.io/academicons/) in your Quarto HTML documents. It provides an `{{&lt; ai &gt;}}` shortcode:\n- Mandatory `&lt;icon-name&gt;`:\n``` markdown\n{{&lt; ai &lt;\ngoogle-scholar \n&gt; &gt;}}\n```\n- Optional `&lt;size=…&gt;`:\n``` markdown\n{{&lt; ai &lt;\ngoogle-scholar \n&gt; &lt;size=…&gt; &gt;}}\n```\n- Optional `&lt;color=…&gt;`:\n``` markdown\n{{&lt; ai &lt;\ngoogle-scholar \n&gt; &lt;color=…&gt; &gt;}}\n```"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "Efficiency - What is it all about?\n\n\n\nnews\n\n\n\n\n\n\n\nEnrico De Monte\n\n\nNov 1, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/metadata/posts/post-with-code/index.html",
    "href": "posts/metadata/posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/metadata/about.html",
    "href": "posts/metadata/about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2023-11-01/index.html",
    "href": "posts/2023-11-01/index.html",
    "title": "Efficiency - What is it all about?",
    "section": "",
    "text": "Efficiency is relative, which means that if a company A compared to company B needs less production inputs, such as work force, financial resources, or energy, to produce a specific product, economists say company A is relatively more efficient. Efficiency, though, is not something that we can observe directly by asking “Hey company, would you mind to tell me how efficient you are relative to all your competitors ?” This is why in economic research people spend their lives to find the best way to estimate firms’ efficiency. So do I, too! But why do we believe it is so important? This blog post is to tell about it!\nLet’s start by workers’ productivity. Imagine you are working with two colleagues in an office. All of you are doing the same task, say editing texts for a newspaper, where you are payed equally by edited words. Your colleague Marie is super productive, during the same time she edits twice as much as words as you do. Your colleague Theo is a daydreamer and edits only half of the amount of words as you do. By that for a given amount of working hours, Marie will always be able to afford a higher standard of living than you and Theo. Also, if Marie decides to stop working after say 6 hours a day, as her earnings allow her to do so, her higher productivity provides her either a higher wage or more leisure time.\nThe manager of the newspaper observers the differences in productivity among you and comes to the conclusion to propose Marie to rise her hourly wage if she takes takes over some of your and Theo’s work. The manager expects, however, to reduce overall costs, that is, the increase in Marie’s workload, accompanied by an increase in her wage, is fully compensated by less but relatively costly working hours of you and Theo. In other words, the manager reallocates some of the work to Marie, to increase what economists call allocative efficiency.\nThe newspaper now incurs lower production costs. As the market for newspaper is highly competitive, especially since online content is way cheaper than print media, the manager decides to fully pass through the lower production costs to the consumer by reducing the price of the newspaper. By Marie’s high productivity, all readers of the newspaper now have less expenses, for her daily newspaper, allowing them to save more money or to consume more of other products. If we think about health care, high-quality food, or education as consumption products, even if only small, the reduction in the newspaper’s price will marginally increase the standard of living of it’s readers. Marie’s higher productivity therefore not only affects her standard of living but also those of many other members of the society.\nBy the reduction in the newspaper’s price, some readers of other newspaper might think about their choice and will probably change their subscription, given the prices of other newspapers keep the same. Put differently, your company will now attract a higher demand and so, it will have to think about how to increase its capacity to meet the higher demand. If we think of a newspaper industry, the company you are working for with Marie and Theo, in that way increases its market share, that is, it produces more newspaper for more readers as it did before. In other words, we observe reallocation of output (number of printed newspapers) from less efficient to a more efficient newspaper company, the one where you work with Marie and Theo. Now, within the newspaper industry, a higher degree of allocative efficiency has been reached, i.e. less costs for a given total amount of production.\nPolicy makers aim to increase allocative efficiency, i.e. allowing the production to shift to most efficient producers, hoping, as mentioned above, that consumers pay lower prices by that, and so increasing their standard of living.\nIn one of my works, for instance, I investigate productivity dynamics of French manufacturing firms, where I found that the reallocation process, i.e. the shift of production from less efficient to more efficient producers over time, slowed significantly down over the past decates. In another work we show that if we could as what economists call a ‘social planner’ redistribute production to the most efficient firms, we could increase total welfare by X%.\nOf course, it’s not all about efficiency in our lives and what increases our standards of living. Policy is asked to set rules ensuring equal rights, a healthy environment, and a save place to live. But within any given world, promoting the reallocation of production to most efficient producers would at least contribute to a higher standard of living by the mentioned reasons."
  },
  {
    "objectID": "posts/metadata/index.html",
    "href": "posts/metadata/index.html",
    "title": "metadata",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/metadata/posts/welcome/index.html",
    "href": "posts/metadata/posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "code.html#data-generating-process",
    "href": "code.html#data-generating-process",
    "title": "Code and Packages",
    "section": "",
    "text": "library(knitr)\nlibrary(latex2exp)\n\nset.seed(49)\n\n# Generate panel structure \nN = 100  \nT = 20   \nNT = N*T\niota_T = rep(1, T)\niota_N = rep(1, N)\ntrend_T = c(1:T)\nind_N = c(1:N)\ncol_T = iota_N %x% trend_T \ncol_N = ind_N %x% iota_T \n\n# Parts of the DGP in Racine (2019, p. 288) \n# Correlation parameters \nrho.xz = 0.2  \nrho.ux = 0.8\nsigma.u = 0.05\n\n# DGP of the quadratic model \ndgp = function(x){x^2}\n\n# Generate endogeneity in x \nv1 = rnorm(NT, 0, 0.8)\nv2 = rnorm(NT, 0, 0.15)\neps = rnorm(NT, 0, sigma.u)\nu = rho.ux*v1 + eps \n\n# Individual fixed effects\nxi_i = runif(N, 0, 1.5)\nxi_i = as.matrix( xi_i %x% iota_T )\n\n# Time fixed effects \ndelta_t = runif(T, 0, 1.7)\ndelta_t = as.matrix( rep( delta_t, N ) )\n\n# Explanatory variable needs to be correlated with\n# the error term as well as with the\n# individual and temporal effects\n\n# Generate \"auxiliary\" explanatory variable\nx = matrix( NT, nrow = NT, ncol = 1 )\n\n# Correlation with individual and temporal effects\nfor(i in 1:NT){\n  x[i,1] = rnorm( 1, mean = (xi_i[i] + delta_t[i]), sd = 1 ) \n}\n\n# Auxiliary variable used to introduce correlation \n# between the instrument and the explanatory variable\nz = rho.xz*x + v2\n\n# Generate explanatory variable also correlated\n# with error term (i.e. E(u*x) != 0)\nx = x + v1\n\n# Dependent variable \ny = as.matrix( dgp(x=x) ) + xi_i + delta_t + u \n\n# Plot the data points along with the true DGP\nplot( x, y,\n      pch = 21, \n      col = \"grey\", \n      ylab = \"Y\", \n      xlab = \"X\" ,\n      main = \"\")\ncurve( dgp(x),\n       col=\"red\", \n       add=TRUE, \n       lty = 2, \n       lwd = 2)\nlegend( -2, 40, \n        legend = c(\"DGP\", \"Observations\"), \n        col = c(\"red\", \"grey\"),\n        lty = c(2, 1),\n        lwd = c(2, 2),\n        cex = 0.8)\n\n\n\n# Covariance matrix \ncovmat = round( cov( cbind( x, z, xi_i, delta_t, u ) ), 2)\ncolnames( covmat ) &lt;- rownames( covmat ) &lt;- c( \"$x_{it}$\", \"$z_{it}$\", \"$\\\\xi_i$\", \"$\\\\delta_t$\", \"$u_{it}$\" )\ncovmat[ lower.tri( covmat, diag = TRUE ) ] =  \"\"\nkable( covmat, \n       caption = \"Covariance matrix of observables and unobservables\" )\n\n\nCovariance matrix of observables and unobservables\n\n\n\n\\(x_{it}\\)\n\\(z_{it}\\)\n\\(\\xi_i\\)\n\\(\\delta_t\\)\n\\(u_{it}\\)\n\n\n\n\n\\(x_{it}\\)\n\n0.27\n0.19\n0.25\n0.52\n\n\n\\(z_{it}\\)\n\n\n0.04\n0.05\n0\n\n\n\\(\\xi_i\\)\n\n\n\n0\n-0.01\n\n\n\\(\\delta_t\\)\n\n\n\n\n0.01\n\n\n\\(u_{it}\\)\n\n\n\n\n\n\n\n\n\n\nTo further investigate the data you might assemble the observable variables with the individual and time identifiers:\n\nData = data.frame( cbind( col_N, col_T, y, x, z ) )\ncolnames( Data ) &lt;- c( \"col_N\", \"col_T\", \"y\", \"x\", \"z\" )"
  },
  {
    "objectID": "code.html#estimation",
    "href": "code.html#estimation",
    "title": "Code and Packages",
    "section": "",
    "text": "Load the functions needed for the nonparametric instrumental estimator with two-way fixed effects. (Download above the file npivfe.R containing the functions.)\n\nsource(\".../npivfe.R\")\n\nUse the function npregivfe provided in the package to estimate the conditional mean function \\(\\varphi\\).\nNote that the estimation routine is very time intensive as it involves several numerical optimizations to obtain optimal bandwidths as well as several iterations of the Landweber-Fridman regularization procedure. However, I’m sure that the way I coded the estimator can be improved significantly to reduce the run time.\n\nres.npivfe = npregivfe( y = y,\n                        x = x,\n                        z = z, \n                        c = 0.5,  \n                        tol = 1,   \n                        N = N,\n                        T = T, \n                        max.iter = 100,\n                        bw_method = \"optimal\",\n                        effects = \"indiv-time\",\n                        type = \"ll\" )\n\nInvestigate the regularized solution path:\n\nlibrary(latex2exp)\nlibrary(colorRamps)\n# (NT x K) matrix, with K the total number of iterations\n# within the Landweber-Fridman estimation procedure. \nphi_k_mat = res.npivfe[[1]]\n\n# Total number of iterations\nk_bar = ncol( phi_k_mat ) \n\n# Final point-wise estimates of the function of interest\nfit.npivfe = phi_k_mat[, k_bar]  \n\n# Plot the initial guess phi_0\nplot( x[order(x)], phi_k_mat[, 1][order(x)],\n      col = blue2green(k_bar)[1],\n      type = \"l\",\n      lwd = 2,\n      ylim = range(y),\n      ylab = \"Y\", xlab = \"X\", main = \"\" ) \n\n# Plot all successive estimates phi_k, k=1,...,k_bar\nfor(k in 1:k_bar){\n  lines( x[order(x)], phi_k_mat[, k][order(x)], \n         type = \"l\",\n         lwd = 2, \n         col =  blue2green(k_bar)[k] )\n}\n# Add the true curve  \ncurve( dgp, \n       min(x), max(x),\n       add = TRUE, \n       col = \"red\", \n       lwd = 2, lty = 2)\nlegend( -2, 40, \n        legend = c(\"DGP\", TeX( '$\\\\hat{\\\\varphi}(x)_{0}$'), \n                  TeX('$\\\\hat{\\\\varphi}(x)_{\\\\bar{k}}$')), \n        col = c(\"red\",blue2green(k_bar)[1], blue2green(k_bar)[k_bar]),\n        lty = c(2, 1, 1),\n        lwd = c(2, 1, 1),\n        cex = 0.65)\n\n\nInvestigate the evolution of the stopping criterion. Note that the number of iterations of the Landweber-Fridman procedure not only vary when changing the constant c but also for some numerical reasons (bandwidths selection). Even though I here also specified c = 0.5, the number of iterations is slightly higher, given by 48 (44 in the paper).\n\n# Store the values of the stopping criterion\nval_crit_k = res.npivfe[[2]]\n\n# Plot the stopping criterion \nplot( 1:k_bar,\n      val_crit_k, type = \"l\", \n      col = \"black\",\n      lwd = 2, \n      main = \"\", \n      xlab = \"k-iterations\",\n      ylab= TeX('$Criterion_k $'))\npoints( k_bar, val_crit_k[k_bar] )"
  },
  {
    "objectID": "code.html#comparison-with-other-estimators",
    "href": "code.html#comparison-with-other-estimators",
    "title": "Code and Packages",
    "section": "",
    "text": "1. Local-linear kernel regression (LL)\nUse the functions npregbw and npreg from the np package to estimate the model by a nonparametric local-linear kernel regression.\n\nlibrary(np)\n# Compute optimal bandwidth \nfit.np_bw = npregbw( xdat = as.vector(x), ydat = as.vector(y), \n                     ckertype=\"gaussian\", \n                     regtype = \"ll\", \n                     bwmethod = \"cv.ls\")\n# Obtain fitted values \nfit.np = fitted( npreg( fit.np_bw ) )\n\n2. Nonparametric locally weighted fixed effects estimator (LMU)\nFirst, use the function LMU.CVCMB from the provided package to obtain optimal bandwidths, using leave-one-out conditional mean based cross-validation (CVCMB). Second, use the function LMU.estimator (Lee et al. 2019) with the previously obtained bandwidths to estimate the function of interest.\n\n# Compute optimal bandwidth\nh_LMU = optim( par = sqrt( var(x) )*NT^(-1/7),\n               LMU.CVCMB, \n               method = \"Nelder-Mead\",\n               control = list(reltol=0.001), \n               x = x,\n               y = y,\n               N = N,\n               T = T,\n               effects = \"indiv-time\",\n               type = \"ll\",\n               jump = 1, \n               c1 = 1/T^2, c2 = 1/((N*T)^2) )$par\n\n# Obtain fitted values\nfit.LMU = LMU.estimator( h = h_LMU,\n                         x = x,\n                         y = y,\n                         N = N,\n                         T = T,\n                         effects = \"indiv-time\",\n                         type = \"ll\",\n                         c1 = c1, c2 = c2)[ , \"mhat\"]\n\n3. Nonparametric IV using the Landweber-Fridman regularization (L-F)\nUse the function npregiv2 from the provided package to estimate the model by nonparametric IV regression (without fixed effects) applying the Landweber-Fridman (L-F) regularization. Note that the function relies on the np package (as it calls the functions npreg and npregbw) and produces very similar results compared to the np::npregiv function (also see Florens et al. 2018).\n\n# Nonparametric IV (without fixed effects)\nres.npivf2 = npregivf2( y = y,\n                        z = x, \n                        w = z, \n                        c = 0.5,\n                        tol=1,\n                        max.iter = 100,\n                        bw_method = \"optimal\" )\n\n# Obtain final estimates by selecting the last column (last iteration)\nfit.npivf2 = res.npivf2[[1]][, ncol( res.npivf2[[1]] ) ]\n\nPlot the results to compare the different estimators.\n\nplot( x, y, \n      pch = 21, \n      col = \"grey\", \n      ylab = \"Y\", \n      xlab = \"X\" ,\n      main = \"\")\nlines( x[order(x)], fit.np[order(x)],\n       col= \"orange\",\n       type = \"l\",\n       lty =1,\n       lwd = 2)\nlines( x[order(x)], fit.npiv2[order(x)],\n       col= \"black\",\n       type = \"l\",\n       lty = 1,\n       lwd = 2)\nlines( x[order(x)], fit.LMU[order(x)],\n       col= \"darkblue\",\n       type = \"l\",\n       lty = 1,\n       lwd = 2)\nlines( x[order(x)], fit.npivfe[order(x)],\n       type = \"l\",\n       col = blue2green(k_bar)[k_bar], \n       lty =1, \n       lwd = 2)\ncurve( dgp(x),\n       col=\"red\", \n       add=TRUE, \n       lty = 2, \n       lwd = 2)\nlegend( -1,40, \n        legend = c(\"DGP\", \"local-linear\", \"L-F\", \"LMU\", \"L-F/LMU\"), \n        col = c(\"red\", \"orange\", \"black\" , \"darkblue\", blue2green(k_bar)[k_bar]),\n        lty = c(2,1,1,1,1), \n        lwd = c(2,2,2,2,2), \n        cex = 0.65 )"
  },
  {
    "objectID": "code.html#bootstrapped-confidence-intervalls",
    "href": "code.html#bootstrapped-confidence-intervalls",
    "title": "Code and Packages",
    "section": "Bootstrapped Confidence Intervalls",
    "text": "Bootstrapped Confidence Intervalls\nAs proposed in the paper, bootstrapped confidence intervalls can be obtained by the application of the wild block bootstrapp. See Appendix B in the paper for more details. A chunk code how the estimates where obtained are provided below. Note that the estimation routine is very time intensive. Pluralizing or using more computers (as I did) would speed up the routine.\n\n# Follow Malikov et al. (2020)  \n# Bootstrap iterations \nB = 400\n\n# Use bandwidths from initial estimation of the conditional mean. \nbws = res.npivfe[[3]]\n\n\n# Compute residuals u_hat \nv_hat = y - fit.npivfe \n\n# Center residuals u_hat\nv_hat_c = as.numeric(scale(v_hat, center = TRUE))\n\n# Create empty lists to store bootstrapp estimates\nphi_hat_boot_list &lt;- list()\n\nfor(b in 1:B){\n  \n  # Generate bootstrap weights \n  # each individual i keeps its weight for all t\n  b_i = sample( c((1+sqrt(5))/2, (1-sqrt(5))/2 ), \n                n, \n                prob = c( ( sqrt( 5 ) - 1 ) / ( 2 * sqrt( 5 ) ), ( sqrt( 5 )+1 ) / ( 2*sqrt( 5 ) ) ),\n                replace = TRUE )\n  \n  # Generate new disturbances\n  v_hat_b = (b_i %x% iota_T) * v_hat_c\n  \n  # Generate new outcome variable\n  y_b = phi_hat + v_hat_b\n  \n  # Re-estimate phi_hat using y_b \n  res.npivfe_b = npivfe(y = y_b, \n                     x = x, \n                     z = z, \n                     N = N, \n                     T = T,\n                     bw_method = \"plug_in\", \n                     effects = \"indiv-time\", \n                     type = \"ll\", \n                     c = 0.5,\n                     tol = 1, \n                     bws = bws,\n                     max.iter = 100)\n  \n  phi_hat_b = res.npivfe_b[[1]][,length(res.npivfe_b[[2]])]\n  \n  phi_hat_boot_list[[b]] &lt;- phi_hat_b\n}\n\n# Bind the results in a (NT x B) matrix.  \nphi_boot = do.call(cbind, phi_hat_boot_list)\n\n# Compute the pointwise 95% confidence intervalls \nphi_025 = apply(phi_boot, 1, quantile, probs=0.025)\nphi_975 = apply(phi_boot, 1, quantile, probs=0.975)\n\n# Plot the estimates along with the confidence intervalls \nplot(x[order(x)], fit.npivfe[order(x)],\n     ylim = c(0,40),\n     type = \"l\",\n     lty = 1,\n     lwd = 2,\n     ylab = \"Y\",\n     xlab = \"X\",\n     main = \"\")\nlines(x[order(x)], phi_025[order(x)],\n      lty = 2, \n      col = \"black\")\nlines(x[order(x)], phi_975[order(x)],\n      lty = 2,\n      col = \"black\")\nlegend(-1, 35, \n       legend = c(TeX( '$\\\\hat{\\\\varphi}(x)_{\\\\bar{k}}$'), \"95 % CI\"), \n       col = c(\"black\", \"black\"),\n       lty = c(1, 2, 2), lwd = c(2, 1, 1),\n       cex = 0.8 )"
  },
  {
    "objectID": "code.html#monte-carlo-simulation",
    "href": "code.html#monte-carlo-simulation",
    "title": "Code and Packages",
    "section": "",
    "text": "Lastly, as shown in the paper, a Monte Carlo simulation demonstrates the finite sample properties of the proposed estimator in comparison with other nonparametric estimators, such as those already presented above (i.e. the LL, LMU, and the L-F estimator). The code below shows how the results were obtained. Running the code, however, is very time intensive, which unfortunately renders the reproduction of the results very costly.\nFollowing Lee et al. (2020), the Monte Carlo simulation is performed for four different data sets with \\(N\\in \\{50,100\\}\\) and \\(T\\in\\{10,20\\}\\). Each of the estimator is applied on each data set 400 times. To reduce run time, this is done by fixing bandwidths to the ones obtained from the first iteration. Subsequently, the Root Mean Squared Error (RMSE) and the Integrated Mean Absolute Error (IMAE) are computed.\nThe function dgpivfe provided in the package allows to generate the above described data by specifying the length of the individual and time dimension, N and T, yielding a data.table object that contains the observables y, x, and z.\n\n# Vectors defining the length of the data sets\nno_indiv = c(50, 50, 100, 100)\nno_time =  c(10, 20, 10, 20)\n\n# Generate matrices to store the values of the RMSE and the IMAE\nSim_res_RMSE_1 = Sim_res_RMSE_2 = Sim_res_RMSE_3 = Sim_res_RMSE_4 = matrix( 0, nrow = rep, ncol = 4 )\ncolnames(Sim_res_RMSE_1) = colnames(Sim_res_RMSE_2) = colnames(Sim_res_RMSE_3) = colnames(Sim_res_RMSE_4) &lt;- c(\"LF/LMU\", \"LF\", \"LMU\", \"LL\")\n\nSim_res_IMAE_1 = Sim_res_IMAE_2 = Sim_res_IMAE_3 = Sim_res_IMAE_4 = matrix( 0, nrow = rep, ncol = 4 )\ncolnames(Sim_res_IMAE_1) = colnames(Sim_res_IMAE_2) = colnames(Sim_res_IMAE_3) = colnames(Sim_res_IMAE_4) &lt;- c(\"LF/LMU\", \"LF\", \"LMU\", \"LL\")\n\ndgp = function(x){ x^2 }\n\n# Monte Carlo repetitions\nrep = 400\n\nfor(d in 1:4){\n  for(i in 1:rep){ \n  # Specify length of the data\n  N = no_indiv[d]; T = no_time[d] ; NT = N*T\n  # Draw the data \n  Data = dgpivfe(N=N, T=T)\n  \n  # 1) The LF/LMU estimator (IV with two-way fixed effects)\n  if(i == 1){ # Bandwidths are fixed to those obtained of the first repetition \n    res.npivfe = npregivfe( y = Data$y,\n                            x = Data$x, \n                            z = Data$z,\n                            N = N,\n                            T = T,\n                            c = 0.8, \n                            tol = 1,\n                            max.iter = 100,\n                            bw_method = \"optimal\" )\n    bws_LF_LMU = phi_hat_LF_LMU[[3]]\n    fit.npivfe =res.npivfe[[1]][, ncol( res.npivfe[[1]] ) ]\n  }else{\n    res.npivfe = res.npivfe( y = Data$y,\n                             x = Data$x,\n                             z = Data$z,\n                             N = N, \n                             T = T,\n                             c = 0.8,\n                             tol=1,\n                             max.iter=100,\n                             bw_method = \"plug_in\",\n                             bws = bws_LF_LMU )\n  }\n  # 2) The LF estimator (nonparametric IV only)\n  if(i == 1){\n    res.npiv2 = npregiv2( y = Data$y,\n                          z = Data$x, \n                          w = Data$z, \n                          c = 0.8,\n                          tol=1,\n                          max.iter = 100,\n                          bw_method = \"optimal\" )\n    bws_LF = res.npiv2[[3]]\n    fit.npivf2 = res.npivf2[[1]][, ncol( res.npivf2[[1]] ) ]\n  }else{\n    res.npiv2 = npivf2( y = Data$y,\n                        z = Data$x,\n                        w = Data$z,\n                        c = 0.8, \n                        tol=1,\n                        max.iter = 100,\n                        bw_method = \"plug_in\",\n                        bws = bws_LF  )\n    fit.npivf2 = res.npivf2[[1]][, ncol(res.npivf2[[1]])]\n    }\n\n  # The LUM estimator (nonparametric two-way fixed effects) \n  if(i==1){\n  bw_LMU = optimize( f = Lee.CVCMB,\n                     interval = c(0,5*sd(Data$x)),\n                     lower = 0,\n                     upper = 5*sd(Data$x),\n                     tol = 0.001,\n                     x = Data$x,\n                     y = Data$y,\n                     N = N,\n                     T = T,\n                     effects = \"indiv-time\",\n                     type = \"ll\",\n                     jump = 1,\n                     c1 = 1/T^2, c2 = 1/((N*T)^2))$minimum\n  \n   fit.LMU = LMU.estimator( h = bw_LMU,\n                            x = Data$x,\n                            y = Data$y,\n                            N = N,\n                            T = T,\n                            effects = \"indiv-time\",\n                            type = \"ll\",\n                            c1 = 1/T^2, c2 = 1/((N*T)^2))[,\"mhat\"]\n  }else{\n   fit.LMU = LMU.estimator( h = bw_LMU,\n                            x = Data$x, \n                            y = Data$y,\n                            N = N,\n                            T = T,\n                            effects = \"indiv-time\",\n                            type = \"ll\",\n                            c1 = 1/T^2, c2 = 1/((N*T)^2))[,\"mhat\"]\n  }\n\n  # The LL estimator (local-linear kernel regression) \n  if(i==1){\n    bw_ll = npregbw( Data$y~Data$x,\n                     ckertype=\"gaussian\" ,\n                     regtype = \"ll\",\n                     bwmethod = \"cv.ls\")$bw \n    fit.LL = fitted( npreg( bws = bw_ll,\n                            tydat = Data$y, \n                            txdat = Data$x ) ) \n  }else{\n    fit.LL = fitted( npreg( bws = bw_ll,  \n                            tydat = Data$y,\n                            txdat = Data$x,\n                            ckertype=\"gaussian\"))\n  }\n  if(d == 1){ \n    # RMSE \n    Sim_res_RMSE_1[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_1[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_1[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_1[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_1[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_1[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_1[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_1[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n  if(d==2){\n    # RMSE \n    Sim_res_RMSE_2[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_2[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_2[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_2[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_2[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_2[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_2[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_2[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n  if(d==3){\n    # RMSE \n    Sim_res_RMSE_3[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_3[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_3[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_3[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_3[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_3[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_3[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_3[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n  if(d==4){\n    # RMSE \n    Sim_res_RMSE_4[i,\"LF/LMU\"] = sqrt( mean( ( fit.npivfe - dgp(Data$x ) )^2 ) )\n    Sim_res_RMSE_4[i,\"LF\"] = sqrt( mean( ( fit.npiv2 - dgp( Data$x ) )^2 ) )\n    Sim_res_RMSE_4[i,\"LMU\"] = sqrt( mean( ( fit.LMU - dgp( Data$x ) )^2) )\n    Sim_res_RMSE_4[i,\"LL\"] = sqrt( mean( ( fit.LL - dgp(Data$x) )^2) )\n    # IMEA \n    Sim_res_IMAE_4[i,\"LF/LMU\"] = mean( abs( fit.npivfe - dgp( Data$x ) ) )\n    Sim_res_IMAE_4[i,\"LF\"] = mean( abs( fit.npiv2 - dgp( Data$x ) ) )\n    Sim_res_IMAE_4[i,\"LMU\"] = mean( abs( fit.LMU - dgp( Data$x ) ) )\n    Sim_res_IMAE_4[i,\"LL\"] = mean( abs( fit.LL - dgp( Data$x ) ) )\n  }\n\n }\n}\n\n# Output table of the MC simulation\ntab.MC = cbind(no_indiv, no_time, rbind( round( colMeans( Sim_res_RMSE_1 ), 3 ), \n                                         round( colMeans( Sim_res_RMSE_2 ), 3 ),\n                                         round( colMeans( Sim_res_RMSE_3 ), 3 ), \n                                         round( colMeans( Sim_res_RMSE_4 ), 3 ) ),\n                                  rbind( round( colMeans( Sim_res_IMAE_1 ), 3 ), \n                                         round( colMeans( Sim_res_IMAE_2 ), 3 ),\n                                         round( colMeans( Sim_res_IMAE_3 ), 3 ), \n                                         round( colMeans( Sim_res_IMAE_4 ), 3 ) ) )\n\ncolnames(tab.MC)[1:2] &lt;- c(\"N\", \"T\")\n\nThe table tab.MC provides the results of the Monte Carlo simulation as shown below, where the columns 3 to 6 and 7 to 10 refer to the RMSE and the IMAE, respectively.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(N\\)\n\\(T\\)\nL-F/LMU\nL-F\nLMU\nLL\nL-F/LMU\nL-F\nLMU\nLL\n\n\n\n\n50\n10\n1.041\n1.664\n1.501\n1.793\n0.350\n1.606\n0.781\n1.675\n\n\n50\n20\n0.309\n1.660\n0.894\n1.797\n0.123\n1.604\n0.760\n1.678\n\n\n100\n10\n0.822\n1.670\n1.258\n1.778\n0.250\n1.609\n0.865\n1.660\n\n\n100\n20\n0.524\n1.663\n0.915\n1.786\n0.116\n1.608\n0.717\n1.668"
  },
  {
    "objectID": "code.html#bootstrapped-confidence-intervals",
    "href": "code.html#bootstrapped-confidence-intervals",
    "title": "Code and Packages",
    "section": "",
    "text": "As proposed in the paper, bootstrapped confidence intervals can be obtained by the application of the wild residual block bootstrap following Malikov et al. (2020) and Azomahou et al. (2006). See Appendix B in the paper for more details. The chunk code provided below shows how the estimates were obtained. Note again that the estimation routine is very time intensive. Hence, parallelizing and/or using more computers (as I did) would be useful speed up the routine.\n\n# Use bandwidths from initial estimation of the conditional mean. \nbws = res.npivfe[[3]]\n\n# Compute residuals u_hat \nv_hat = y - fit.npivfe \n\n# Center residuals u_hat\nv_hat_c = as.numeric(scale(v_hat, center = TRUE))\n\n# Create empty lists to store bootstrap estimates\nphi_hat_boot_list &lt;- list()\n\n# Bootstrap iterations \nB = 400\nfor(b in 1:B){\n  \n  # Generate bootstrap weights \n  # each individual i keeps its weight for all t\n  b_i = sample( x = c( (1+sqrt(5))/2, (1-sqrt(5))/2 ), \n                size = N, \n                prob = c( ( sqrt( 5 ) - 1 ) / ( 2 * sqrt( 5 ) ), ( sqrt( 5 )+1 ) / ( 2*sqrt( 5 ) ) ),\n                replace = TRUE )\n  \n  # Generate new disturbances\n  v_hat_b = (b_i %x% iota_T) * v_hat_c\n  \n  # Generate new outcome variable\n  y_b = phi_hat + v_hat_b\n  \n  # Re-estimate phi_hat using y_b \n  res.npivfe_b = npregivfe( y = y_b, \n                            x = x, \n                            z = z, \n                            N = N, \n                            T = T,\n                            bw_method = \"plug_in\", \n                            effects = \"indiv-time\", \n                            type = \"ll\", \n                            c = 0.5,\n                            tol = 1, \n                            bws = bws,\n                            max.iter = 100)\n  \n  phi_hat_b = res.npivfe_b[[1]][, ncol( res.npivfe_b[[1]] ) ]\n  \n  phi_hat_boot_list[[b]] &lt;- phi_hat_b\n}\n\n# Bind the results in a (NT x B) matrix.  \nphi_boot = do.call( cbind, phi_hat_boot_list )\n\n# Compute the point-wise 95% confidence intervals \nphi_025 = apply( phi_boot, 1, quantile, probs=0.025 )\nphi_975 = apply( phi_boot, 1, quantile, probs=0.975 )\n\n# Plot the estimates along with the confidence intervals \nplot( x[order(x)], fit.npivfe[order(x)],\n      ylim = c(0, 40),\n      type = \"l\",\n      lty = 1,\n      lwd = 2,\n      ylab = \"Y\",\n      xlab = \"X\",\n      main = \"\")\nlines( x[order(x)], phi_025[order(x)],\n       lty = 2, \n       col = \"black\")\nlines( x[order(x)], phi_975[order(x)],\n       lty = 2,\n       col = \"black\")\nlegend( -1, 35, \n        legend = c(TeX( '$\\\\hat{\\\\varphi}(x)_{\\\\bar{k}}$'), \"95 % CI\"), \n        col = c(\"black\", \"black\"),\n        lty = c(1, 2, 2), lwd = c(2, 1, 1),\n        cex = 0.8 )"
  },
  {
    "objectID": "code.html#references",
    "href": "code.html#references",
    "title": "Code and Packages",
    "section": "",
    "text": "Azomahou, T., Laisney, F., & Van, P. N. (2006). Economic development and CO2 emissions: A nonparametric panel approach. Journal of Public Economics, 90(6-7), 1347-1363.\nFlorens, J. P., Racine, J. S., & Centorrino, S. (2018). Nonparametric instrumental variable derivative estimation. Journal of Nonparametric Statistics, 30(2), 368-391.\nLee, Y., Mukherjee, D., & Ullah, A. (2019). Nonparametric estimation of the marginal effect in fixed-effect panel data models. Journal of Multivariate Analysis, 171, 53-67.\nMalikov, E., Zhao, S., & Kumbhakar, S. C. (2020). Estimation of firm‐level productivity in the presence of exports: Evidence from China’s manufacturing. Journal of Applied Econometrics, 35(4), 457-480.\nRacine, J. S. (2019). An introduction to the advanced theory of nonparametric econometrics: A replicable approach using R. Cambridge University Press."
  },
  {
    "objectID": "code.html#package-download",
    "href": "code.html#package-download",
    "title": "Code and Packages",
    "section": "",
    "text": "Download the file npivfe containing an R file with all functions needed to apply the approach described in the paper as well as a readme file providing a description of the functions."
  },
  {
    "objectID": "code.html#model-description",
    "href": "code.html#model-description",
    "title": "Code and Packages",
    "section": "",
    "text": "Below we generate data of the two-way fixed effects panel model, given by \\[ Y_{it} = \\varphi(X_{it}) + \\xi_i + \\delta_t + U_{it},\\] where \\(X_{it}\\) denotes the endogenous dependent variable that is correlated with both the unobserved individual and temporal fixed effects, \\(\\xi_i\\) and \\(\\delta_t\\), and with the error term \\(U_{it}\\). Further, we generate a valid instrument \\(Z_{it}\\), satisfying \\[\nE(U_{it} | Z_{it}) = 0 \\hspace{4mm} \\text{and} \\hspace{4mm} cor(X_{it},Z_{it}) \\neq 0.  \n\\] Note that to stay consistent with the variable convention used in the code, the notation used here changes slightly w.r.t. the one presented in paper. More precisely, while in the paper \\(Z_{it}\\) and \\(W_{it}\\) denote the explanatory and the instrumental variable, in the code these variables refer to x and z, respectively."
  },
  {
    "objectID": "index.html#news-blog",
    "href": "index.html#news-blog",
    "title": "Enrico De Monte",
    "section": "News Blog",
    "text": "News Blog\n\nJanuary, 8th 2024. Replication notebook available here for Nonparametric instrumental regression with two-way fixed effects, Journal of Econometric Methods.\n\n\nOctober, 5th 2023. Publication of my paper Nonparametric instrumental regression with two-way fixed effects in the Journal of Econometric Methods.\n\n\nSeptember, 22nd 2023. Winning the Leibniz Competition Fund for the 3-year research project “High Growth Entrepreneurship, Innovation, and the Transformation of the Economy. See more here."
  },
  {
    "objectID": "index.html#welcome-to-my-personal-website",
    "href": "index.html#welcome-to-my-personal-website",
    "title": "Enrico De Monte",
    "section": "",
    "text": "I’m researcher at ZEW - Leibniz Centre for European Economic Research. My research interests lie in the fields of industrial organisation and applied econometrics.\nTopics I work on comprise productivity growth, competition, market power, business dynamism, and innovation. Understanding the interrelation between these concepts provides insights to the evolution of an economy in terms of technological change and the process of job creation and destruction, which plays an important role for our standard of living. Contributing to designing efficient policies to support the latter is the ultimate purpose of my research."
  }
]